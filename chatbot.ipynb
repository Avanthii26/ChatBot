{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTh8JhDjLdbq",
        "outputId": "543e07dd-b051-48a9-ea04-488c95c13d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langgraph langchain_google_genai fastapi uvicorn pyngrok nest-asyncio streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing_extensions import TypedDict\n",
        "from typing import Annotated\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Set your Google API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyB9OYLj7B3reaOPuMgD2mWUX9uaL_qqxAc\"\n",
        "\n",
        "# Initialize AI model\n",
        "gemini_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "# Define ChatState for LangGraph\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Chatbot function\n",
        "def chatbot(state: ChatState) -> ChatState:\n",
        "    return {\"messages\": gemini_llm.invoke(state[\"messages\"])}\n",
        "\n",
        "# Build graph\n",
        "graph_builder = StateGraph(ChatState)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        ""
      ],
      "metadata": {
        "id": "9foU55C3Loj0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import threading\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio for Jupyter compatibility\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Kill any existing processes\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"-f\", \"streamlit\"], check=False)\n",
        "    subprocess.run([\"pkill\", \"-f\", \"uvicorn\"], check=False)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Close existing ngrok tunnels\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"Setting up LangGraph Chatbot \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH3lD_fhLwA2",
        "outputId": "fcdb84ba-6c9e-4402-b2f9-ec714c0b2a8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up LangGraph Chatbot \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "backend_code = \"\"\"\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing_extensions import TypedDict\n",
        "from typing import Annotated\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set your Google API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyB9OYLj7B3reaOPuMgD2mWUX9uaL_qqxAc\"\n",
        "\n",
        "# Initialize AI model\n",
        "try:\n",
        "    gemini_llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-lite\",\n",
        "        temperature=0.3\n",
        "    )\n",
        "    logger.info(\"Gemini model initialized successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to initialize Gemini: {e}\")\n",
        "    raise\n",
        "\n",
        "# Define ChatState for LangGraph\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Chatbot function\n",
        "def chatbot(state: ChatState) -> ChatState:\n",
        "    try:\n",
        "        response = gemini_llm.invoke(state[\"messages\"])\n",
        "        return {\"messages\": response}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Chatbot error: {e}\")\n",
        "        raise\n",
        "\n",
        "# Build graph\n",
        "try:\n",
        "    graph_builder = StateGraph(ChatState)\n",
        "    graph_builder.add_node(\"chatbot\", chatbot)\n",
        "    graph_builder.add_edge(START, \"chatbot\")\n",
        "    graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "    # Add memory\n",
        "    memory = MemorySaver()\n",
        "    graph = graph_builder.compile(checkpointer=memory)\n",
        "    logger.info(\"LangGraph compiled successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to build graph: {e}\")\n",
        "    raise\n",
        "\n",
        "# FastAPI app\n",
        "app = FastAPI(\n",
        "    title=\"LangGraph Chatbot API\",\n",
        "    version=\"1.0.0\",\n",
        "    description=\"A chatbot powered by Google Gemini and LangGraph\"\n",
        ")\n",
        "\n",
        "# CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class UserMessage(BaseModel):\n",
        "    message: str\n",
        "    thread_id: str = \"1\"\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    reply: str\n",
        "    status: str = \"success\"\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"message\": \"LangGraph Chatbot API is running!\",\n",
        "        \"status\": \"healthy\",\n",
        "        \"endpoints\": [\"/chat\", \"/health\"]\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"service\": \"LangGraph Chatbot\"}\n",
        "\n",
        "@app.post(\"/chat\", response_model=ChatResponse)\n",
        "async def chat(msg: UserMessage):\n",
        "    try:\n",
        "        logger.info(f\"Received message: {msg.message[:50]}...\")\n",
        "\n",
        "        config = {\"configurable\": {\"thread_id\": msg.thread_id}}\n",
        "        state = ChatState(messages=[{\"role\": \"user\", \"content\": msg.message}])\n",
        "\n",
        "        # Invoke the graph in a separate thread to avoid async issues\n",
        "        import asyncio\n",
        "        result = await asyncio.to_thread(lambda: graph.invoke(state, config))\n",
        "        reply = result[\"messages\"][-1].content\n",
        "\n",
        "        logger.info(f\"Generated reply: {reply[:50]}...\")\n",
        "        return ChatResponse(reply=reply)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Chat endpoint error: {str(e)}\")\n",
        "        return ChatResponse(\n",
        "            reply=f\"I apologize, but I encountered an error: {str(e)}\",\n",
        "            status=\"error\"\n",
        "        )\n",
        "\"\"\"\n",
        "\n",
        "# Write to file\n",
        "with open(\"backend.py\", \"w\") as f:\n",
        "    f.write(backend_code)\n",
        "\n",
        "print(\"backend.py created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKLeDWLwL4Bi",
        "outputId": "762fa9fd-05cc-4d1d-b8ef-92f8b36ea54d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "backend.py created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "print(\"Starting FastAPI backend\")\n",
        "\n",
        "backend_process = subprocess.Popen([\n",
        "    'python', '-c',\n",
        "    '''\n",
        "import uvicorn\n",
        "uvicorn.run(\"backend:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "'''\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Wait for backend to start\n",
        "time.sleep(8)\n",
        "\n",
        "# Check if backend is running\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:8000/health\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Backend is running\")\n",
        "    else:\n",
        "        print(f\"Backend returned status: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"Backend health check failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw2D0DipMduh",
        "outputId": "193c8e8f-bef2-4588-bde0-48080d2c7b3b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FastAPI backend\n",
            "Backend health check failed: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a5bfb702ed0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 33GNqbyjjUumFM9dwVJMWGgM49E_7m6FZZ5EB3N8b77zHdWue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmJuiWwkMsGQ",
        "outputId": "f4251219-11a5-4329-d9fe-4041a789b8fd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a tunnel on port 8000\n",
        "fastapi_tunnel = ngrok.connect(8000)\n",
        "\n",
        "# Extract the public URL\n",
        "fastapi_url = str(fastapi_tunnel).split(' -> ')[0].replace('NgrokTunnel: \"', '').replace('\"', '')\n",
        "\n",
        "print(f\"FastAPI Backend URL: {fastapi_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11OILxQCMyF1",
        "outputId": "5e5fda1d-44e4-4b25-b9dc-5f13e365806b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI Backend URL: https://supervenient-zaiden-beauteously.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit_code = f'''\n",
        "import streamlit as st\n",
        "import requests\n",
        "import json\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"LangGraph Chatbot\",\n",
        "    page_icon=\"�4f8\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Custom CSS for better styling\n",
        "st.markdown(\"\"\"\n",
        "\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.markdown(' LangGraph Chatbot', unsafe_allow_html=True)\n",
        "st.markdown('Powered by Google Gemini + LangGraph', unsafe_allow_html=True)\n",
        "\n",
        "FASTAPI_URL = \"{fastapi_url}\"\n",
        "\n",
        "# Initialize session state\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"backend_status\" not in st.session_state:\n",
        "    st.session_state.backend_status = \"unknown\"\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"🔧 Control Panel\")\n",
        "\n",
        "    # Connection test\n",
        "    if st.button(\" Test Backend Connection\"):\n",
        "        with st.spinner(\"Testing...\"):\n",
        "            try:\n",
        "                response = requests.get(f\"{{FASTAPI_URL}}/health\", timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    st.session_state.backend_status = \"connected\"\n",
        "                    st.markdown(' Backend Connected!', unsafe_allow_html=True)\n",
        "                    st.json(response.json())\n",
        "                else:\n",
        "                    st.session_state.backend_status = \"error\"\n",
        "                    st.markdown(f' Error: {{response.status_code}}', unsafe_allow_html=True)\n",
        "            except Exception as e:\n",
        "                st.session_state.backend_status = \"error\"\n",
        "                st.markdown(f' Connection Failed: {{str(e)}}', unsafe_allow_html=True)\n",
        "\n",
        "    # Show current status\n",
        "    status_color = \"success\" if st.session_state.backend_status == \"connected\" else \"error\"\n",
        "    st.markdown(f\"**Status:** {{st.session_state.backend_status.title()}}\", unsafe_allow_html=True)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(f\"**Backend URL:**\")\n",
        "    st.code(\"{{FASTAPI_URL}}\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    # Clear chat\n",
        "    if st.button(\"Clear Chat History\"):\n",
        "        st.session_state.messages = []\n",
        "        st.success(\"Chat cleared!\")\n",
        "        st.rerun()\n",
        "\n",
        "    # Chat statistics\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(f\"**Total Messages:** {{len(st.session_state.messages)}}\")\n",
        "\n",
        "    if len(st.session_state.messages) > 0:\n",
        "        user_msgs = len([m for m in st.session_state.messages if m[\"role\"] == \"user\"])\n",
        "        bot_msgs = len([m for m in st.session_state.messages if m[\"role\"] == \"assistant\"])\n",
        "        st.markdown(f\"- User: {{user_msgs}}\")\n",
        "        st.markdown(f\"- Assistant: {{bot_msgs}}\")\n",
        "\n",
        "# Main chat interface\n",
        "chat_container = st.container()\n",
        "\n",
        "with chat_container:\n",
        "    # Display chat messages\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    # Show welcome message if no messages\n",
        "    if len(st.session_state.messages) == 0:\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(\"\"\"\n",
        "             **Hello! I'm your LangGraph chatbot powered by Google Gemini.**\n",
        "\n",
        "            I can help you with:\n",
        "            -  Answering questions\n",
        "            -  Providing explanations\n",
        "            -  Having conversations\n",
        "            -  Problem-solving\n",
        "\n",
        "            Just type your message below to get started!\n",
        "            \"\"\")\n",
        "\n",
        "# Chat input\n",
        "if prompt := st.chat_input(\"Ask me anything...\", key=\"chat_input\"):\n",
        "    # Add user message\n",
        "    st.session_state.messages.append({{\"role\": \"user\", \"content\": prompt}})\n",
        "\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Get bot response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        message_placeholder = st.empty()\n",
        "\n",
        "        with st.spinner(\" Thinking...\"):\n",
        "            try:\n",
        "                # Make request to backend\n",
        "                response = requests.post(\n",
        "                    f\"{{FASTAPI_URL}}/chat\",\n",
        "                    json={{\"message\": prompt, \"thread_id\": \"1\"}},\n",
        "                    headers={{\"ngrok-skip-browser-warning\": \"true\"}},\n",
        "                    timeout=45\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    bot_reply = result.get(\"reply\", \"No response generated.\")\n",
        "                    message_placeholder.markdown(bot_reply)\n",
        "                    st.session_state.messages.append({{\"role\": \"assistant\", \"content\": bot_reply}})\n",
        "                else:\n",
        "                    error_msg = f\"Backend Error {{response.status_code}}: {{response.text}}\"\n",
        "                    message_placeholder.error(error_msg)\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                message_placeholder.error(\" **Request timed out.** The model might be processing a complex query. Please try again.\")\n",
        "            except requests.exceptions.ConnectionError:\n",
        "                message_placeholder.error(\" **Connection Error.** Please check if the backend is running and try the 'Test Connection' button.\")\n",
        "            except Exception as e:\n",
        "                message_placeholder.error(f\" **Unexpected Error:** {{str(e)}}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\n",
        "    \"\"\n",
        "    \"LangGraph Chatbot | Built with FastAPI + Streamlit + Google Gemini\"\n",
        "    \"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "'''\n",
        "\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"Created app.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG6aqgUKM6T7",
        "outputId": "5ca66b75-e31e-462a-dcf6-68f09f494ce8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Starting Streamlit app...\")\n",
        "\n",
        "streamlit_process = subprocess.Popen([\n",
        "    'streamlit', 'run', 'app.py',\n",
        "    '--server.port', '8501',\n",
        "    '--server.headless', 'true',\n",
        "    '--browser.gatherUsageStats', 'false',\n",
        "    '--server.address', '0.0.0.0'\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "time.sleep(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN6bFVl4NMSt",
        "outputId": "5e9d408d-a0a6-48a1-9807-055e4a8e3870"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting Streamlit app...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Create Streamlit tunnel\n",
        "streamlit_tunnel = ngrok.connect(8501)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" SERVICES STARTED SUCCESSFULLY\")\n",
        "print(\"=\"*60)\n",
        "print(f\" FastAPI Backend URL : {fastapi_tunnel}\")\n",
        "print(f\" Streamlit App URL  : {streamlit_tunnel}\")\n",
        "print(\"=\"*60)\n",
        "print(\" Open the Streamlit link above to start chatting\")\n",
        "print(\" Use the sidebar option 'Test Connection' to check backend\")\n",
        "print(\" Chatbot remembers conversation history automatically\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Background service monitor\n",
        "def check_services():\n",
        "    while True:\n",
        "        try:\n",
        "            time.sleep(60)  # Check every minute\n",
        "            # Check backend process\n",
        "            if backend_process.poll() is not None:\n",
        "                print(\" FastAPI backend stopped unexpectedly\")\n",
        "            # Check frontend process\n",
        "            if streamlit_process.poll() is not None:\n",
        "                print(\" Streamlit app stopped unexpectedly\")\n",
        "\n",
        "            # Check ngrok tunnels\n",
        "            tunnels = ngrok.get_tunnels()\n",
        "            if len(tunnels) < 2:\n",
        "                print(\" One or more ngrok tunnels are disconnected\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Service monitor error: {e}\")\n",
        "\n",
        "# Start monitor in background\n",
        "status_thread = threading.Thread(target=check_services, daemon=True)\n",
        "status_thread.start()\n",
        "\n",
        "print(\"\\n All services are live and being monitored\")\n",
        "print(\" Status will be checked every 60 seconds\")\n",
        "print(\" Press Ctrl+C to shut everything down safely\")\n",
        "\n",
        "# Keep alive loop\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(30)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n Stopping services...\")\n",
        "    try:\n",
        "        backend_process.terminate()\n",
        "        streamlit_process.terminate()\n",
        "        ngrok.kill()\n",
        "        print(\" All services stopped cleanly\")\n",
        "    except:\n",
        "        print(\" Cleanup incomplete, some services may still be running\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cex7QSp1NX6W",
        "outputId": "5557e939-54a0-4a13-9185-2ae73a2941c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " SERVICES STARTED SUCCESSFULLY\n",
            "============================================================\n",
            " FastAPI Backend URL : NgrokTunnel: \"https://supervenient-zaiden-beauteously.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
            " Streamlit App URL  : NgrokTunnel: \"https://supervenient-zaiden-beauteously.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "============================================================\n",
            " Open the Streamlit link above to start chatting\n",
            " Use the sidebar option 'Test Connection' to check backend\n",
            " Chatbot remembers conversation history automatically\n",
            "============================================================\n",
            "\n",
            " All services are live and being monitored\n",
            " Status will be checked every 60 seconds\n",
            " Press Ctrl+C to shut everything down safely\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n",
            " One or more ngrok tunnels are disconnected\n"
          ]
        }
      ]
    }
  ]
}